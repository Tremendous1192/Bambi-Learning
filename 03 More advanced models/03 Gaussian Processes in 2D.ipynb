{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article demonstrates how to use Bambi with Gaussian Processes with 2 dimensional predictors. Bambi supports Gaussian Processes through the low-rank approximation known as [Hilbert Space Gaussian Processes. For references see Hilbert Space Methods for Reduced-Rank Gaussian Process Regression](https://arxiv.org/abs/1401.5508) and [Practical Hilbert Space Approximate Bayesian Gaussian Processes for Probabilistic Programming](https://arxiv.org/abs/2004.11408).\n",
    "\n",
    "If you prefer a video format, have a look at [Introduction to Hilbert Space GPs in PyMC](https://www.youtube.com/watch?v=ri5sJAdcYHk&ab_channel=PyMCDevelopers) given by Bill Engels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import bambi as bmb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to showcase Bambi’s support for Gaussian Processes on two-dimensional data using the HSGP approximation.\n",
    "\n",
    "To achieve this, we begin by creating a matrix of coordinates that will serve as the locations where we measure the values of a continuous response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.linspace(0, 10, 12)\n",
    "x2 = np.linspace(0, 10, 12)\n",
    "xx, yy = np.meshgrid(x1, x2)\n",
    "X = np.column_stack([xx.flatten(), yy.flatten()])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotropic samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In modeling multi-dimensional data with a Gaussian Process, we must choose between using an isotropic or an anisotropic Gaussian Process. An isotropic GP applies the same degree of smoothing to all predictors and is rotationally invariant. On the other hand, an anisotropic GP assigns different degrees of smoothing to each predictor and is not rotationally invariant.\n",
    "\n",
    "Furthermore, as the ```hsgp()``` function allows for the creation of separate GP contribution terms for the levels of a categorical variable through its ```by``` argument, we also examine both single-group and multiple-group scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a covariance kernel using ```ExpQuad``` from the ```gp``` submodule in PyMC. Note that the lengthscale and amplitude for both dimensions are 2 and 1.2, respectively. Then, we simply use NumPy to get a random draw from the 144-dimensional multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144,) (144, 144)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.default_rng(1234)\n",
    "\n",
    "ell = 2\n",
    "cov = 1.2 * pm.gp.cov.ExpQuad(2, ls=ell)\n",
    "K = cov(X).eval()\n",
    "mu = np.zeros(X.shape[0])\n",
    "print(mu.shape, K.shape)\n",
    "\n",
    "f = rng.multivariate_normal(mu, K)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xx, yy, c=f, s=900, marker=\"s\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Bambi works with long-format data frames, we need to reshape our data before creating the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": np.tile(xx.flatten(), 1),\n",
    "        \"y\": np.tile(yy.flatten(), 1), \n",
    "        \"outcome\": f.flatten()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s construct the model. The only notable distinction from the one-dimensional case is that we provide two unnamed arguments to the ```hsgp()``` function, representing the predictors on each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Formula: outcome ~ 0 + hsgp(x, y, c=1.5, m=10)\n",
       "        Family: gaussian\n",
       "          Link: mu = identity\n",
       "  Observations: 144\n",
       "        Priors: \n",
       "    target = mu\n",
       "        HSGP contributions\n",
       "            hsgp(x, y, c=1.5, m=10)\n",
       "                cov: ExpQuad\n",
       "                sigma ~ Exponential(lam: 3.0)\n",
       "                ell ~ InverseGamma(mu: 2.0, sigma: 0.2)\n",
       "        \n",
       "        Auxiliary parameters\n",
       "            sigma ~ HalfNormal(sigma: 2.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_hsgp = {\n",
    "    \"sigma\": bmb.Prior(\"Exponential\", lam=3),\n",
    "    \"ell\": bmb.Prior(\"InverseGamma\", mu=2, sigma=0.2),\n",
    "}\n",
    "priors = {\n",
    "    \"hsgp(x, y, c=1.5, m=10)\": prior_hsgp, \n",
    "    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=2)\n",
    "}\n",
    "model = bmb.Model(\"outcome ~ 0 + hsgp(x, y, c=1.5, m=10)\", data, priors=priors)\n",
    "model.set_alias({\"hsgp(x, y, c=1.5, m=10)\": \"hsgp\"})\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters ```c``` and ```m``` of the HSGP aproximation are specific to each dimension, and can have different values for each. However, as we are passing scalars instead of sequences, Bambi will internally recycle them, causing the HSGP approximation to use the same values of ```c``` and ```m``` for both dimensions.\n",
    "\n",
    "Let’s build the internal PyMC model and create a graph to have a visual representation of the relationships between the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 11.0.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"435pt\" height=\"580pt\"\n",
       " viewBox=\"0.00 0.00 434.78 579.95\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 575.95)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-575.95 430.78,-575.95 430.78,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>clusterhsgp_weights_dim (100)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M136.45,-348.63C136.45,-348.63 294.45,-348.63 294.45,-348.63 300.45,-348.63 306.45,-354.63 306.45,-360.63 306.45,-360.63 306.45,-551.95 306.45,-551.95 306.45,-557.95 300.45,-563.95 294.45,-563.95 294.45,-563.95 136.45,-563.95 136.45,-563.95 130.45,-563.95 124.45,-557.95 124.45,-551.95 124.45,-551.95 124.45,-360.63 124.45,-360.63 124.45,-354.63 130.45,-348.63 136.45,-348.63\"/>\n",
       "<text text-anchor=\"middle\" x=\"230.58\" y=\"-355.83\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">hsgp_weights_dim (100)</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster__obs__ (144)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174.45,-8C174.45,-8 256.45,-8 256.45,-8 262.45,-8 268.45,-14 268.45,-20 268.45,-20 268.45,-328.63 268.45,-328.63 268.45,-334.63 262.45,-340.63 256.45,-340.63 256.45,-340.63 174.45,-340.63 174.45,-340.63 168.45,-340.63 162.45,-334.63 162.45,-328.63 162.45,-328.63 162.45,-20 162.45,-20 162.45,-14 168.45,-8 174.45,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"221.08\" y=\"-15.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">__obs__ (144)</text>\n",
       "</g>\n",
       "<!-- hsgp_sigma -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>hsgp_sigma</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"57.45\" cy=\"-515.29\" rx=\"57.45\" ry=\"40.66\"/>\n",
       "<text text-anchor=\"middle\" x=\"57.45\" y=\"-526.74\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">hsgp_sigma</text>\n",
       "<text text-anchor=\"middle\" x=\"57.45\" y=\"-510.24\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"57.45\" y=\"-493.74\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Exponential</text>\n",
       "</g>\n",
       "<!-- hsgp_weights -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>hsgp_weights</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"260.58,-438.63 170.33,-438.63 170.33,-381.13 260.58,-381.13 260.58,-438.63\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-421.33\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">hsgp_weights</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-404.83\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-388.33\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Deterministic</text>\n",
       "</g>\n",
       "<!-- hsgp_sigma&#45;&gt;hsgp_weights -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>hsgp_sigma&#45;&gt;hsgp_weights</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M98.97,-487.12C118.63,-474.25 142.33,-458.75 163.2,-445.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"164.85,-448.19 171.3,-439.78 161.01,-442.33 164.85,-448.19\"/>\n",
       "</g>\n",
       "<!-- hsgp_ell -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>hsgp_ell</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"371.45\" cy=\"-515.29\" rx=\"55.33\" ry=\"40.66\"/>\n",
       "<text text-anchor=\"middle\" x=\"371.45\" y=\"-526.74\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">hsgp_ell</text>\n",
       "<text text-anchor=\"middle\" x=\"371.45\" y=\"-510.24\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"371.45\" y=\"-493.74\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">InvGamma</text>\n",
       "</g>\n",
       "<!-- hsgp_ell&#45;&gt;hsgp_weights -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>hsgp_ell&#45;&gt;hsgp_weights</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.87,-487.39C311.54,-474.58 288.2,-459.11 267.58,-445.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"269.54,-442.54 259.27,-439.93 265.67,-448.37 269.54,-442.54\"/>\n",
       "</g>\n",
       "<!-- sigma -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>sigma</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"336.45\" cy=\"-198.48\" rx=\"57.98\" ry=\"40.66\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.45\" y=\"-209.93\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sigma</text>\n",
       "<text text-anchor=\"middle\" x=\"336.45\" y=\"-193.43\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"336.45\" y=\"-176.93\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- outcome -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>outcome</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"215.45\" cy=\"-81.16\" rx=\"44.72\" ry=\"40.66\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-92.61\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">outcome</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-76.11\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-59.61\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Normal</text>\n",
       "</g>\n",
       "<!-- sigma&#45;&gt;outcome -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>sigma&#45;&gt;outcome</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M302.43,-165.05C287.31,-150.64 269.44,-133.61 253.84,-118.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"256.67,-116.61 247.02,-112.24 251.84,-121.68 256.67,-116.61\"/>\n",
       "</g>\n",
       "<!-- hsgp -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>hsgp</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"260.58,-332.63 170.33,-332.63 170.33,-275.13 260.58,-275.13 260.58,-332.63\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-315.33\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">hsgp</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-298.83\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-282.33\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Deterministic</text>\n",
       "</g>\n",
       "<!-- hsgp_weights&#45;&gt;hsgp -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>hsgp_weights&#45;&gt;hsgp</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.45,-380.74C215.45,-369.53 215.45,-356.42 215.45,-344.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"218.95,-344.56 215.45,-334.56 211.95,-344.56 218.95,-344.56\"/>\n",
       "</g>\n",
       "<!-- hsgp_weights_raw -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>hsgp_weights_raw</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"215.45\" cy=\"-515.29\" rx=\"82.91\" ry=\"40.66\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-526.74\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">hsgp_weights_raw</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-510.24\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-493.74\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Normal</text>\n",
       "</g>\n",
       "<!-- hsgp_weights_raw&#45;&gt;hsgp_weights -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>hsgp_weights_raw&#45;&gt;hsgp_weights</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.45,-474.31C215.45,-466.46 215.45,-458.23 215.45,-450.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"218.95,-450.42 215.45,-440.42 211.95,-450.42 218.95,-450.42\"/>\n",
       "</g>\n",
       "<!-- mu -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>mu</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"260.58,-227.23 170.33,-227.23 170.33,-169.73 260.58,-169.73 260.58,-227.23\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-209.93\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">mu</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-193.43\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"215.45\" y=\"-176.93\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Deterministic</text>\n",
       "</g>\n",
       "<!-- hsgp&#45;&gt;mu -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>hsgp&#45;&gt;mu</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.45,-274.9C215.45,-263.75 215.45,-250.72 215.45,-238.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"218.95,-239 215.45,-229 211.95,-239 218.95,-239\"/>\n",
       "</g>\n",
       "<!-- mu&#45;&gt;outcome -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>mu&#45;&gt;outcome</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.45,-169.27C215.45,-158.44 215.45,-145.73 215.45,-133.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"218.95,-133.78 215.45,-123.78 211.95,-133.78 218.95,-133.78\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1edb97fbc10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.build()\n",
    "model.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we quickly fit the model and show a traceplot to explore the posterior and spot any issues with the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "'numpyro_nuts' method has not been implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m idata \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumpyro_nuts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(idata\u001b[38;5;241m.\u001b[39msample_stats\u001b[38;5;241m.\u001b[39mdiverging\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\treme\\.conda\\envs\\pymc_env\\Lib\\site-packages\\bambi\\models.py:348\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, draws, tune, discard_tuned_samples, omit_offsets, include_mean, include_response_params, inference_method, init, n_init, chains, cores, random_seed, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_mean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been replaced by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_response_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not going to work in the future\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    345\u001b[0m     )\n\u001b[0;32m    346\u001b[0m     include_response_params \u001b[38;5;241m=\u001b[39m include_mean\n\u001b[1;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43momit_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momit_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_response_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_response_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\treme\\.conda\\envs\\pymc_env\\Lib\\site-packages\\bambi\\backend\\pymc.py:150\u001b[0m, in \u001b[0;36mPyMCModel.run\u001b[1;34m(self, draws, tune, discard_tuned_samples, omit_offsets, include_response_params, inference_method, init, n_init, chains, cores, random_seed, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_laplace(draws, omit_offsets, include_response_params)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minference_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method has not been implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: 'numpyro_nuts' method has not been implemented"
     ]
    }
   ],
   "source": [
    "idata = model.fit(inference_method=\"numpyro_nuts\", target_accept=0.9, num_chains=4)\n",
    "print(idata.sample_stats.diverging.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata, \n",
    "    var_names=[\"hsgp_sigma\", \"hsgp_ell\", \"sigma\"], \n",
    "    backend_kwargs={\"layout\": \"constrained\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don’t see any divergences. However, the autocorrelation in the chains for the covariance function parameters, along with the insufficient mixing, indicates that there may be an issue with the prior specification of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goal of the notebook is to simply show what features Bambi supports and how to use them, we won’t further investigate these issues. However, such posteriors shouldn’t be considered in any serious application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, the notebook will follow the same structure as the one already shown, which consists of\n",
    "\n",
    "* Data simulation with some specific settings\n",
    "* Creation of the Bambi model\n",
    "* Building of the internal PyMC model and visualization of the graph\n",
    "* Model fit and inspection of the traceplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple groups - same covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario we have multiple groups that share the same covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(123)\n",
    "\n",
    "ell = 2\n",
    "cov = 1.2 * pm.gp.cov.ExpQuad(2, ls=ell)\n",
    "K = cov(X).eval()\n",
    "mu = np.zeros(X.shape[0])\n",
    "\n",
    "f = rng.multivariate_normal(mu, K, 3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.scatter(xx, yy, c=f[i], s=320, marker=\"s\")\n",
    "    ax.grid(False)\n",
    "    ax.set_title(f\"Group {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": np.tile(xx.flatten(), 3),\n",
    "        \"y\": np.tile(yy.flatten(), 3),\n",
    "        \"group\": np.repeat(list(\"ABC\"), 12 * 12),\n",
    "        \"outcome\": f.flatten()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we don’t modify anything substantial in the call to ```hsgp()``` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_hsgp = {\n",
    "    \"sigma\": bmb.Prior(\"Exponential\", lam=3),\n",
    "    \"ell\": bmb.Prior(\"InverseGamma\", mu=2, sigma=0.2),\n",
    "}\n",
    "priors = {\n",
    "    \"hsgp(x, y, by=group, c=1.5, m=10)\": prior_hsgp, \n",
    "    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=2)\n",
    "}\n",
    "model = bmb.Model(\"outcome ~ 0 + hsgp(x, y, by=group, c=1.5, m=10)\", data, priors=priors)\n",
    "model.set_alias({\"hsgp(x, y, by=group, c=1.5, m=10)\": \"hsgp\"})\n",
    "print(model)\n",
    "model.build()\n",
    "model.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = model.fit(inference_method=\"numpyro_nuts\", target_accept=0.9, num_chains=4)\n",
    "print(idata.sample_stats.diverging.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata, \n",
    "    var_names=[\"hsgp_sigma\", \"hsgp_ell\", \"sigma\"], \n",
    "    backend_kwargs={\"layout\": \"constrained\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have three groups, we only have one ```hsgp_sigma``` and one ```hsgp_ell``` for all groups. This is because, by default, the HSGP contributions by groups use the same instance of the covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple groups - different covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we have multiple groups. But this time, each group has specific values for the amplitude and the lengthscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "\n",
    "sigmas = [1.2, 1.5, 1.8]\n",
    "ells = [1.5, 2, 3]\n",
    "\n",
    "samples = []\n",
    "for sigma, ell in zip(sigmas, ells):\n",
    "    cov = sigma * pm.gp.cov.ExpQuad(2, ls=ell)\n",
    "    K = cov(X).eval()\n",
    "    mu = np.zeros(X.shape[0])\n",
    "    samples.append(rng.multivariate_normal(mu, K))\n",
    "\n",
    "f = np.stack(samples)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.scatter(xx, yy, c=f[i], s=320, marker=\"s\")\n",
    "    ax.grid(False)\n",
    "    ax.set_title(f\"Group {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": np.tile(xx.flatten(), 3),\n",
    "        \"y\": np.tile(yy.flatten(), 3),\n",
    "        \"group\": np.repeat(list(\"ABC\"), 12 * 12),\n",
    "        \"outcome\": f.flatten()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situations like this, we can tell Bambi not to use the same covariance function for all the groups with ```share_cov=False``` and Bambi will create a separate instance for each group, resulting in group specific estimates of the amplitude and the lengthscale.\n",
    "\n",
    "Notice, however, we’re still using the same kind of covariance function, which in this case is ```ExpQuad```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_hsgp = {\n",
    "    \"sigma\": bmb.Prior(\"Exponential\", lam=3),\n",
    "    \"ell\": bmb.Prior(\"InverseGamma\", mu=2, sigma=0.2),\n",
    "}\n",
    "priors = {\n",
    "    \"hsgp(x, y, by=group, c=1.5, m=10, share_cov=False)\": prior_hsgp, \n",
    "    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=2)\n",
    "}\n",
    "model = bmb.Model(\n",
    "    \"outcome ~ 0 + hsgp(x, y, by=group, c=1.5, m=10, share_cov=False)\", \n",
    "    data, \n",
    "    priors=priors\n",
    ")\n",
    "model.set_alias({\"hsgp(x, y, by=group, c=1.5, m=10, share_cov=False)\": \"hsgp\"})\n",
    "print(model)\n",
    "model.build()\n",
    "model.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the all the HSGP related parameters gained the new dimension ```hsgp_by```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = model.fit(inference_method=\"numpyro_nuts\", target_accept=0.9, num_chains=4)\n",
    "print(idata.sample_stats.diverging.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata, \n",
    "    var_names=[\"hsgp_sigma\", \"hsgp_ell\", \"sigma\"], \n",
    "    backend_kwargs={\"layout\": \"constrained\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous case, now there are three ```hsgp_sigma``` and three ```hsgp_ell``` parameters, one per group. We can see them in different colors in the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anisotropic samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part we repeat exactly the same that we did for the isotropic case. First, we start with a single group. Then, we continue with multiple groups that share the covariance function. And finally, multiple groups with different covariance functions. The main difference is that we use ```iso=False```, which asks to use an anisotropic GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)\n",
    "\n",
    "ell = [2, 0.9]\n",
    "cov = 1.2 * pm.gp.cov.ExpQuad(2, ls=ell)\n",
    "K = cov(X).eval()\n",
    "mu = np.zeros(X.shape[0])\n",
    "\n",
    "f = rng.multivariate_normal(mu, K)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (4.5, 4.5))\n",
    "ax.scatter(xx, yy, c=f, s=900, marker=\"s\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": np.tile(xx.flatten(), 1),\n",
    "        \"y\": np.tile(yy.flatten(), 1), \n",
    "        \"outcome\": f.flatten()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_hsgp = {\n",
    "    \"sigma\": bmb.Prior(\"Exponential\", lam=3),\n",
    "    \"ell\": bmb.Prior(\"InverseGamma\", mu=2, sigma=0.2),\n",
    "}\n",
    "priors = {\n",
    "    \"hsgp(x, y, c=1.5, m=10, iso=False)\": prior_hsgp, \n",
    "    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=2)\n",
    "}\n",
    "model = bmb.Model(\"outcome ~ 0 + hsgp(x, y, c=1.5, m=10, iso=False)\", data, priors=priors)\n",
    "model.set_alias({\"hsgp(x, y, c=1.5, m=10, iso=False)\": \"hsgp\"})\n",
    "print(model)\n",
    "model.build()\n",
    "model.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is only one group in this case, the graph includes a ```hsgp_var``` dimension. This dimension represents the variables in the HSGP component, indicating that there is one lengthscale parameter per variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = model.fit(inference_method=\"numpyro_nuts\", target_accept=0.9, num_chains=4)\n",
    "print(idata.sample_stats.diverging.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata, \n",
    "    var_names=[\"hsgp_sigma\", \"hsgp_ell\", \"sigma\"], \n",
    "    backend_kwargs={\"layout\": \"constrained\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple groups - same covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(123)\n",
    "\n",
    "ell = [2, 0.9]\n",
    "cov = 1.2 * pm.gp.cov.ExpQuad(2, ls=ell)\n",
    "K = cov(X).eval()\n",
    "mu = np.zeros(X.shape[0])\n",
    "\n",
    "f = rng.multivariate_normal(mu, K, 3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.scatter(xx, yy, c=f[i], s=320, marker=\"s\")\n",
    "    ax.grid(False)\n",
    "    ax.set_title(f\"Group {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": np.tile(xx.flatten(), 3),\n",
    "        \"y\": np.tile(yy.flatten(), 3),\n",
    "        \"group\": np.repeat(list(\"ABC\"), 12 * 12),\n",
    "        \"outcome\": f.flatten()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_hsgp = {\n",
    "    \"sigma\": bmb.Prior(\"Exponential\", lam=3),\n",
    "    \"ell\": bmb.Prior(\"InverseGamma\", mu=2, sigma=0.2),\n",
    "}\n",
    "priors = {\n",
    "    \"hsgp(x, y, by=group, c=1.5, m=10, iso=False)\": prior_hsgp, \n",
    "    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=2)\n",
    "}\n",
    "model = bmb.Model(\"outcome ~ 0 + hsgp(x, y, by=group, c=1.5, m=10, iso=False)\", data, priors=priors)\n",
    "model.set_alias({\"hsgp(x, y, by=group, c=1.5, m=10, iso=False)\": \"hsgp\"})\n",
    "print(model)\n",
    "model.build()\n",
    "model.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = model.fit(inference_method=\"numpyro_nuts\", target_accept=0.9, num_chains=4)\n",
    "print(idata.sample_stats.diverging.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata, \n",
    "    var_names=[\"hsgp_sigma\", \"hsgp_ell\", \"sigma\"], \n",
    "    backend_kwargs={\"layout\": \"constrained\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple groups - different covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "\n",
    "sigmas = [1.2, 1.5, 1.8]\n",
    "ells = [[1.5, 0.8], [2, 1.5], [3, 1]]\n",
    "\n",
    "samples = []\n",
    "for sigma, ell in zip(sigmas, ells):\n",
    "    cov = sigma * pm.gp.cov.ExpQuad(2, ls=ell)\n",
    "    K = cov(X).eval()\n",
    "    mu = np.zeros(X.shape[0])\n",
    "    samples.append(rng.multivariate_normal(mu, K))\n",
    "\n",
    "f = np.stack(samples)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.scatter(xx, yy, c=f[i], s=320, marker=\"s\")\n",
    "    ax.grid(False)\n",
    "    ax.set_title(f\"Group {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": np.tile(xx.flatten(), 3),\n",
    "        \"y\": np.tile(yy.flatten(), 3),\n",
    "        \"group\": np.repeat(list(\"ABC\"), 12 * 12),\n",
    "        \"outcome\": f.flatten()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_hsgp = {\n",
    "    \"sigma\": bmb.Prior(\"Exponential\", lam=3),\n",
    "    \"ell\": bmb.Prior(\"InverseGamma\", mu=2, sigma=0.2),\n",
    "}\n",
    "priors = {\n",
    "    \"hsgp(x, y, by=group, c=1.5, m=10, iso=False, share_cov=False)\": prior_hsgp, \n",
    "    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=2)\n",
    "}\n",
    "model = bmb.Model(\n",
    "    \"outcome ~ 0 + hsgp(x, y, by=group, c=1.5, m=10, iso=False, share_cov=False)\", \n",
    "    data, \n",
    "    priors=priors\n",
    ")\n",
    "model.set_alias({\"hsgp(x, y, by=group, c=1.5, m=10, iso=False, share_cov=False)\": \"hsgp\"})\n",
    "print(model)\n",
    "model.build()\n",
    "model.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = model.fit(inference_method=\"numpyro_nuts\", target_accept=0.9, num_chains=4)\n",
    "print(idata.sample_stats.diverging.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata, \n",
    "    var_names=[\"hsgp_sigma\", \"hsgp_ell\", \"sigma\"], \n",
    "    backend_kwargs={\"layout\": \"constrained\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex example: Poisson likelihood with group-specific effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this final demonstration we’re going to use a simulated dataset where the outcome is a count variable. For the predictors, we have the location in terms of the latitude and longitude, as well as other variables such as the year of the measurement, the site where the measure was made, and one continuous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/poisson_data.csv\")\n",
    "data[\"Year\"] = pd.Categorical(data[\"Year\"])\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the outcome variable by location and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for i, (ax, year) in enumerate(zip(axes, [2015, 2016])):\n",
    "    mask = data[\"Year\"] == year\n",
    "    x = data.loc[mask, \"Lat\"]\n",
    "    y = data.loc[mask, \"Lon\"]\n",
    "    count = data.loc[mask, \"Count\"]\n",
    "    ax.scatter(x, y, c=count, s=30, marker=\"s\")\n",
    "    ax.set_title(f\"Year {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s not much we can conclude from here but it’s not a problem. The most relevant part of the example is not the data itself, but how to use Bambi to include GP components in a complex model.\n",
    "\n",
    "It’s very easy to create a model that uses both regular common and group-specific predictors as well as a GP contribution term. We just add them to the model formula, treat ```hsgp()``` as any other call, and that’s it!\n",
    "\n",
    "Below we have common effects for the Year, the interaction between X1 and Year, and group-specific intercepts by Site. Finally, we add ```hsgp()``` as any other call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"Count ~ 0 + Year + X1:Year + (1|Site) + hsgp(Lon, Lat, by=Year, m=5, c=1.5)\"\n",
    "model = bmb.Model(formula, data, family=\"poisson\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use an alias to make the graph representation more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_alias({\"hsgp(Lon, Lat, by=Year, m=5, c=1.5)\": \"gp\"})\n",
    "model.build()\n",
    "model.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let’s fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = model.fit(inference_method=\"numpyro_nuts\", target_accept=0.99, num_chains=4)\n",
    "print(idata.sample_stats.diverging.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata, \n",
    "    var_names=[\"gp_sigma\", \"gp_ell\", \"gp_weights\"], \n",
    "    backend_kwargs={\"layout\": \"constrained\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the posteriors for the ```gp_weights``` are all centered at zero. This is a symptom of the absence of any spatial effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata, \n",
    "    var_names=[\"Year\", \"X1:Year\"], \n",
    "    backend_kwargs={\"layout\": \"constrained\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata, \n",
    "    var_names=[\"1|Site\", \"1|Site_sigma\"], \n",
    "    backend_kwargs={\"layout\": \"constrained\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
